\documentclass[10pt,svgnames,ignorenonframetext,final]{beamer}
% \usepackage{polyglossia}
% \setmainlanguage{english}
\usepackage{mathtools}
\usepackage{booktabs}
\input{troll_notations.tex}
\usepackage{colortbl}
\usepackage{appendixnumberbeamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
% titleformat plain=allcaps,
%titleformat frame=smallcaps,
\usetheme[numbering=fraction,progressbar=frametitle,block=fill,]{metropolis}
\definecolor{lightyellow}{RGB}{255,240,210}
\setbeamercolor{frametitle}{bg=lightyellow,fg=DarkOrange}
\setbeamerfont{frametitle}{size=\LARGE,shape=\scshape}
\setbeamertemplate{frametitle}[default][center]
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
% \iffalse
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% \fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight0.8\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\usepackage[citestyle=numeric-comp,bibstyle=ieee,isbn=false,maxnames=1,minnames=1,sorting=none,backend=biber,defernumbers=true]{biblatex}
\AtEveryBibitem{
   \clearfield{arxivId}
   % \clearfield{booktitle}
   \clearfield{doi}
   \clearfield{eprint}
   \clearfield{eventdate}
   \clearfield{isbn}
   \clearfield{issn}
   % \clearfield{journaltitle}
   \clearfield{month}
   % \clearfield{number}
   % \clearfield{pages}
   \clearfield{series}
   % \clearfield{url}
   \clearfield{urldate}
   \clearfield{venue}
   % \clearfield{volume}
   \clearlist{location} % alias to field 'address'
   \clearlist{publisher}
   \clearname{editor}
}
\addbibresource{../trolls/clean.bib}
\addbibresource{trolls.bib}

% Comment these out if you don't want a slide with just the
% part/section/subsection/subsubsection title:
\iffalse
\AtBeginPart{
  \let\insertpartnumber\relax
  \let\partname\relax
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \let\insertsectionnumber\relax
    \let\sectionname\relax
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \let\insertsubsectionnumber\relax
  \let\subsectionname\relax
  \frame{\subsectionpage}
}
\fi


\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\providecommand{\largelist}{%
  \setlength{\itemsep}{8pt}\setlength{\parskip}{3pt}}
\setcounter{secnumdepth}{0}


\tikzset{
  every overlay node/.style={
    % draw=black,fill=white,rounded corners,
    fill=white!98!black, anchor=north west,
  },
}
% Usage:
% \tikzoverlay at (-1cm,-5cm) {content};
% or
% \tikzoverlay[text width=5cm] at (-1cm,-5cm) {content};
\def\tikzoverlay{%
   \tikz[baseline,overlay]\node[every overlay node]
}%

\AtBeginSection{}
\title{Edge Sign Prediction in Social Networks}
\author{Géraud Le Falher --- Inria Lille Nord Europe, \textsc{Magnet} Team\\
  Joint work with Nicolò Cesa-Bianchi, Claudio Gentile and Fabio Vitale}
\date{June 16, 2016}

\begin{document}

\frame{\titlepage}


\section{Problem and Motivations}\label{introduction}

\begin{frame}{Directed signed social networks}

  \begin{itemize}
    \largelist
    \item
      Directed signed social networks have the usual positive relations,
      driven by the \emph{homophily} assumption,
    \item
      but also \alert{negative relations}, e.g.\@ distrust, enemyship.
    \item
      This gives rise to \textbf{new problems}.
    \item
      For instance, observing some signs, can we predict the remaining ones?
  \end{itemize}

\end{frame}

\begin{frame}{Motivations}

  Being able to predict edge signs let us solve \textbf{practical, real world problems}:

  \begin{itemize}
    \largelist
  \item
    ``Frenemy'' detection~\autocite{frenemy12};
  \item
    Automatic moderation of large scale online interactions;
  \item
    Cyber bullying prevention, at school or in online games~\autocite{CyberbullyingCHI15}.
\end{itemize}

\end{frame}

\begin{frame}{Contributions}

  \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \largelist
  \item
    A \alert{generative model} based on trollness and trustworthiness, justifying
    existing heuristics and providing a \alert{new principled predictor}
    % (through Bayes optimal classifier);
  \item
    A maximum likelihood approximation by a \alert{label propagation algorithm},
    leveraging a reduction from \alert{edge to node classification}
  \item
    A \alert{natural complexity measure} leading to an \alert{efficient online algorithm}
\end{enumerate}

\end{frame}

%% PART 2

  \begin{frame}
    \frametitle{Outline}
    \tableofcontents%[currentsection]
  \end{frame}

\section{Notations, Generative Model and Problem Reduction}\label{setting-and-model}

\begin{frame}{Notations}

% \begin{itemize}
%   \tightlist
% \item
  \onslide<1->{
\(G=(V, E)\) is a directed graph with \textbf{no side information} but full topology.
}
% \item
%   Each directed edge \((i,j)\) is labeled \(y_{i,j} \in \{-1, +1\}\).
  % TODO Check whether Y is used in the main part
  % The labels are collectively referred to as \(Y\).
% \end{itemize}

  \begin{center}
    \begin{tikzpicture}[auto,scale=1.2, every node/.style={transform shape}]
      \tikzset{>=latex}
      \tikzstyle{peers}=[draw,circle,black,minimum width=9mm,inner sep=0,fill opacity=.0, text opacity=1]
      \tikzstyle{edge}=[line width=0.8pt,color=Black,-{Latex[length=3mm,width=1.0mm,angle'=40]}]
      \tikzstyle{elabel}=[color=Black]
      \tikzstyle{posc}=[color=Green]
      \tikzstyle{negc}=[color=red]

      \node<1->[peers] (i) at (0,0) {\LARGE $i$};
      \node<1->[peers] (j) at (3.5,-0.66) {\Large $j$};
           <1->
      \draw<1->[edge] (i) -- node [posc,xshift=-2mm] {\Large $+$}                      (3, -2);
      \draw<1->[edge] (i) edge  node [posc,xshift=-2mm] {\Large $\textcolor{Black}{y_{ij}}=\textcolor{Green}{+}$} (j);
      \draw<1->[edge] (i) edge  node [posc,xshift=2mm] {\Large $+$}                    (3, 0.66);
      \draw<1->[edge, densely dashed] (i)   edge   node [negc,xshift=2mm] {\Large $-$} (3, 2);
           <1->
      \draw<1->[edge, densely dashed] (-3, -2) --   node [negc,yshift=-3mm] {\Large $-$} (i);
      \draw<1->[edge, densely dashed] (-3, -1) --   node [negc,yshift=-1mm] {\Large $-$} (i);
      \draw<1->[edge, densely dashed] (-3, 0)  --  node [negc] {\Large $-$} (i);
      \draw<1->[edge,]                (-3, 1) --   node [posc,xshift=-4mm] {\Large $+$} (i);
      \draw<1->[edge,]                (-3, 2) --   node [posc,below,pos=0.2] {\Large $+$} (i);
      \node<2->[fill=white!90!DarkOrange,fill opacity=.9,text opacity=1,rounded corners,inner sep=1pt] at (1,-1.5) {$tr(i)=\frac{1}{4}$};
      \node<2->[fill=white!90!Blue,fill opacity=.5,text opacity=1,rounded corners,inner sep=1pt] at (-1.0,1.7) {$un(i)=\frac{3}{5}$};
    \end{tikzpicture}
  \end{center}
  \onslide<2->{ The \alert{trollness} of $i$ $tr(i)$ is its fraction of negative outgoing links, its
\alert{untrustworthiness} \(un(i)\) is its fraction of negative incoming
links.}

\end{frame}

\begin{frame}{Generative model}
  \begin{center}
    \begin{tikzpicture}[auto,scale=1.0, every node/.style={transform shape}]
      \tikzstyle{peers}=[draw,circle,black,minimum width=9mm,inner sep=0,fill opacity=.0, text opacity=1]
      \tikzstyle{edge}=[line width=0.5pt,color=Black,-{Latex[length=4mm,width=1.5mm,angle'=40]}]
    \node[] (mu) at (3,2) {$\mu(p, q)$ is an arbitrary prior distribution over $[0, 1]\times [0,1]$};
      \node[peers,] (i) at (0,0) {$i$};
    \node[] (iparams) at (1.2,1.1) {$(\outqt{p_i}, q_i) \sim \mu(p, q)$};

      \node[peers,] (j) at (7,0) {$j$};
    \node[] (jparams) at (8.2,1.1) {$(p_j, \inqt{q_j}) \sim \mu(p, q)$};

      \draw<1-1>[edge,opacity=0] (i) edge node [below,yshift=-2mm,opacity=0] {$\mathbb{P}(y_{ij}=+1) = \frac{1}{2}\left(\outqt{p_i}+\inqt{q_j}\right)$} (j);
      \draw<2->[edge,opacity=1] (i) edge node [below,yshift=-2mm,opacity=1] {$\mathbb{P}(y_{ij}=+1) = \frac{1}{2}\left(\outqt{p_i}+\inqt{q_j}\right)$} (j);
    \end{tikzpicture}
  \end{center}
  \onslide<3->{%
    The Bayes optimal prediction for \(y_{i,j}\) is thus
  $$y^*(i,j) = \sgn\left(\Pr\big( y_{i,j} = +1 \big) - \tfrac{1}{2}\right)$$}
\end{frame}

\begin{frame}{Edge-to-node reduction: construction}

  \begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \centering \includegraphics[height=.8\textheight]{g_latex-crop}~\\
      \onslide<1->{\textbf{(a)}~$G$}
    \end{subfigure}~
    \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \onslide<2>{
      \centering
        \includegraphics<2>[height=.8\textheight]{intermediate_gprime}~\\
        \onslide<2>{\hspace{3em}\textbf{(b)}~$G'$}
      }
      \onslide<3>{
      \centering
        \includegraphics<3>[height=.8\textheight]{gprime_latex-crop}~\\
        \onslide<3>{\hspace{3em}\textbf{(b)}~$G'$}
      }
    \end{subfigure}
  \end{figure}

\end{frame}

\begin{frame}{Edge-to-node reduction: propagation}
  \begin{center}
    \includegraphics<1>[height=.9\textheight]{propa_01.pdf}
    \includegraphics<2>[height=.9\textheight]{propa_02.pdf}
  \end{center}
\end{frame}



%% PART 3


\section{Batch Learning: Active and Passive}\label{batch-setting}
  \begin{frame} \frametitle{Outline} \tableofcontents[currentsection] \end{frame}

\begin{frame}{Batch settings}

Given a graph \(G\) labeled by our generative model, we observe a training set $\Etrain$.

We present two methods to predict the labels of $E \setminus \Etrain$:
\begin{itemize}
  \largelist
\item An approximation of the Bayes optimal predictor in an \alert{active setting}
\item An approximation of Maximal Likelihood parameters in a \alert{passive setting}
\end{itemize}

\end{frame}

%%  PART ACTIVE
\begin{frame}{Approximation to Bayes via Active Learning}

  \onslide<1->{
    The complementary to 1 of trollness and untrustworthiness (estimated on $E_0$) are used as proxy
    for $p_i$ and $q_j$ so that
  % We use \(1-\htr(i)\) and \(1-\hun(j)\) as proxy for \(p_i\) and \(q_j\) {\small (where ``hat''
  % quantities are computed on $\Etrain$)} and predict the sign of $(i,j)$ as:

  \vspace{-4mm}
\begin{equation*}
  \label{eq:predictor}
  \yhat(i,j)=\sgn\Big(\underbrace{\outqt{\big(1-\htr(i)\big)} + \inqt{\big(1-\hun(j)\big)}
  -\biasqt{\tau}}_{\Large \approx \frac{1}{2}\left(\outqt{p_i}+\inqt{q_j}\right) = \mathbb{P}(y_{ij}=+1)} - \tfrac{1}{2} \Big)
\end{equation*}
}

  \onslide<2->{
\outqt{\(1-\htr(i)\)} is an indirect observation of \outqt{$p_i$}
\tikzoverlay[text width=3cm] at (2.2cm,1.2cm) { \includegraphics[width=3cm]{qbar} };
}
  \onslide<3->{
\\ Letting $$\biasqt{\qbar_i}=\frac{1}{\dout(i)}\sum_{j\, \text{s.t.} (i,j)\in E}q_j$$
we have $$\outqt{1-\htr(i)} \approx
\frac{1}{2}\left(\outqt{p_i}+\biasqt{\qbar_i}\right)\quad\text{and}\quad\inqt{1-\hun(j)} \approx
\frac{1}{2}\left(\inqt{q_j}+\biasqt{\pbar_j}\right)$$
}
  \onslide<4->{%
Thus we need to subtract $$\biasqt{\tau} =
\biasqt{\frac{1}{2}\left(\mu_p+\mu_q\right)}$$ as \biasqt{$\pbar_j$} and \biasqt{$\qbar_i$}
concentrates around their mean \biasqt{$\mu_p$} and \biasqt{$\mu_q$}.
}

\end{frame}


\begin{frame}{Active algorithm}
  \begin{center}
    \begin{tikzpicture}[auto]
      \tikzstyle{peers}=[draw,circle,black,minimum width=11mm,inner sep=0,fill opacity=.0, text opacity=1]
      \tikzstyle{edge}=[line width=0.5pt,color=Black,-{Latex[length=3mm,width=1.0mm,angle'=40]}]
      \node[peers] (i) at  (-1,0) {\Large $i$};
      \node[peers] (j) at  (8,0) {\Large $j$};
      \node<1-1>[opacity=0] (tr) at (-1.5,1) {\large $\widehat{tr}(i)=\frac{1}{3}$};
      \node<1-3>[opacity=0] (tau) at (3.5,3) {\Large $\widehat{\tau}=\frac{5}{7}$};
      \node<1->[Black] (Q) at (-1, 3) {\Large $Q=3$};
      \draw<1->[edge] (i) -- (j);

      \draw<1-1>[edge] (i) -- (0.5,3);
      \draw<1->[edge] (i) -- (1.5,2);
      \draw<1-1>[edge] (i) -- (3.5,2.5);
      \draw<1->[edge] (i) -- (1.5,-2);
      \draw<1-1>[edge] (i) -- (0.5,-3);

      \draw<1-1>[edge] (j) -- (7,3.3);
      \draw<1->[edge] (6,3) -- (j);
      \draw<1-2>[edge] (6,-3) -- (j);
      \draw<1-2>[edge] (8,3) -- (j);
      \draw<1-2>[edge] (8,-3) -- (j);

      \draw<2->[edge,DarkOrange,line width=1pt,] (i) -- node [posc,] {\Large $+$} (0.5,3);
      \draw<2->[edge,DarkOrange,line width=1pt,] (i) -- node [posc,below] {\Large $+$}(3.5,2.5);
      \draw<2->[edge,DarkOrange,line width=1pt,densely dashed] (i) -- node [negc,] {\Large $-$}(0.5,-3);
      \node<2->[DarkOrange] (tr) at (-1.5,1) {\large $\widehat{tr}(i)=\frac{1}{3}$};

      \draw<2->[edge,DarkOrange,line width=1pt,] (j) -- node [posc,] {\Large $+$} (7,3.3);

      \draw<3->[edge,line width=1pt,Blue,] (8,3) -- node [posc,] {\Large $+$}(j);
      \draw<3->[edge,line width=1pt,Blue,] (6,-3) -- node [posc,above] {\Large $+$}(j);
      \draw<3->[edge,line width=1pt,Blue,densely dashed] (8,-3) -- node [negc,] {\Large $-$}(j);
      \node<3->[Blue] (un) at (6,.8) {\large $\widehat{un}(j)=\frac{1}{3}$};
      \node<4->[Teal] (tau) at (3.5,3) {\Large $\widehat{\tau}=\frac{5}{7}$};

      \node<5->[Black] (tt) at (3.5,.5) {predict $y_{ij}$ as};
      \node<5->[Black] (eq) at (3.5,-.5) {$\yhat(i,j)=\sgn\Big(\outqt{\big(1-\htr(i)\big)} + \inqt{\big(1-\hun(j)\big)}
  -\biasqt{\tauhat} - \tfrac{1}{2} \Big)$};
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{How much sampling is needed?}

  \onslide<1->{Setting $$Q = \frac{1}{2\ve^2}\ln\frac{4|V|}{\delta}$$ we query
  $\Theta\big(|V|\ln|V|\big)$ edges.}

  \onslide<2->{
This is enough to guarantee that
$$\left|\Big[\outqt{\big(1-\htr(i)\big)} + \inqt{\big(1-\hun(j)\big)} -\biasqt{\tauhat} \Big] -
\Big[\frac{\outqt{p_i}+\inqt{q_j}}{2}\Big]\right| \leq 8\epsilon$$
% \yhat(i,j) - y^*(i,j)\right| \leq 8\epsilon $
\alert{holds with probability at least $1-10\delta$} simultaneously for all
non-queried edges $(i,j) \in E$ such that $\dout(i),\din(j) \ge Q$.

Correct prediction as long as $\Pr(y_{i,j}=+1)$ is bounded away from $\tfrac{1}{2}$.}

\end{frame}

\begin{frame}{Maximum Likelihood in Passive Setting}

  \begin{itemize}[<+->]
    \largelist
  \item
    The training set \(E_0\) has a likelihood of $\Pr\left(E_0\,|\, \{p_i, q_i\}_{i=1}^{|V|}\right)$.

  \item
    We would like to \alert{approximate \(y^*(i,j)\)} by resorting to a maximum
    likelihood estimator of the parameters \(\{p_i, q_i\}_{i=1}^{|V|}\)
    based on \(E_0\).

  \item 
    The gradient of the log-likelihood function w.r.t.\@ $p_\ell$ is
    \begin{equation*}
      \sum_{\ell,j\in E_0; y_{\ell j}=+1} \frac{1}{p_\ell+q_j} \quad-
      \sum_{\ell,j\in E_0; y_{\ell j}=-1} \frac{1}{2-p_\ell-q_j}
    \end{equation*}
\end{itemize}

\end{frame}


\begin{frame}{Maximum Likelihood Approximation}

% Unfortunately, setting those derivative to zero is difficult because the
  \begin{itemize}[<+->]
    \largelist
  \item
    Those equations are \textbf{non linear}, thus we
approximate them further.

  \item
This approximation is equivalent to setting to zero the gradient
w.r.t.~\((\bp,\bq) =\{p_i, q_i\}_{i=1}^{|V|}\) of the quadratic function
\[f_{E_0}(\bp,\bq) = \sum_{(i,j) \in E_0} \Bigl(\underbrace{\frac{p_i+q_j}{2}}_{\in [0,1]} -
\underbrace{\frac{1+y_{i,j}}{2}}_{\in [0,1]} \Bigr)^2\]

%TODO to use a label propagation approach that hopefully take advantage of the graph sparse
%connectivity, we add the same function on the test set
  \item We follow a label propagation approach by \textbf{making the test labels appear} and 
    % Finally, to make \textbf{all variables appear},
    minimizing \alert{\(f_{E_0}(\bp,\bq) + f_{E\setminus
    E_0}(\bp,\bq, {y_{i,j}})\)} w.r.t. both \((\bp,\bq)\) and all \(y_{i,j} \in [-1,+1]\), for \((i,j)\in
E\setminus E_0\).

\end{itemize}

\end{frame}


\begin{frame}{Label propagation: regularized objective}
  We use a weighted version of G' with negative edges, which introduces an extra regularization
  term.


  %TODO we use a weighted version of G' with negative edges and this introduce an extra
  %regularization term
% In practice, we exploit our edge to node reduction by \alert{minimizing a
% regularized version} of the previous objective through a \alert{label
% propagation method on a weighted version of \(G'\)} called \(G''\):
  \vspace{-5mm}
\begin{equation*}
  \underbrace{ \widehat{f}\big(\bp,\bq,{y_{i,j}}_{(i,j) \in E\setminus E_0}\big)}_{\text{energy
  function on $G''$}} =
  f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq, {y_{i,j}}) + \widetilde{\mathrm{regul}}
\end{equation*}

\begin{columns}
\begin{column}{0.6\textwidth}
  \begin{center}
  \vspace{-5mm}
   \includegraphics[height=.65\textheight]{gsecond_latex-crop}
  \end{center}
\end{column}
\begin{column}{0.4\textwidth}  %%<--- here
  \onslide<2->{
  We run $\operatorname{diameter}(G)$ iterations of label propagation and use a binary threshold over the estimated $y_{i,j}$ to predict signs.
}
\end{column}
\end{columns}
\end{frame}

\section{Online Learning}\label{online-learning}
  \begin{frame} \frametitle{Outline} \tableofcontents[currentsection] \end{frame}

\begin{frame}{Setting}

The signs are \textbf{adversarial} rather than generated by our model.

At each round, the learner is asked to predict one label, which is then revealed to him and the
procedure repeats.

\end{frame}

\begin{frame}{Labeling regularity}

% Inspired by trollness and trustworthiness, we define the regularity of a labeling \(Y\).

Letting $Y$ be the vector of all labels, $\Psiout(i, Y)$ is the number of least used label outgoing
from $i$, and $\Psiout(Y) = \sum_{i \in V} \Psiout(i,Y)$.

Likewise for incoming edges, \(\Psiin(Y) = \sum_{j \in V} \Psiin(j,Y)\) and finally
\(\Psi_G(Y) = \min\big\{\Psiin(Y), \Psiout(Y)\big\}\).

% This can be read from the cutsize of \(G'\)
\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.6\textwidth}
    \centering
    \vspace{-2cm}
    \begin{tabular}{lcccc|l}
      \toprule
      node $i$        & $1$ & $2$ & $3$ & $4$ &                  \\
      \midrule
      $\Psiout(i, Y)$ & 0   & 0   & 1   & 0   & $\Psiout(Y) = 1$  \\
      $\Psiin(i, Y)$  & 1   & 0   & 0   & 1   & $\Psiin(Y) = 2$ \\
      \bottomrule
    \end{tabular}
  \end{subfigure}~
  \begin{subfigure}[t]{0.35\textwidth}
    \centering \includegraphics[height=.35\textheight]{g_latex-crop}
  \end{subfigure}
\end{figure}

\end{frame}

\begin{frame}{Online algorithm and bounds}
  \begin{itemize}[<+->]
    \largelist
  \item 
    Our algorithm is a combination of Randomized Weighted Majority instances built on top of each other. 

  \item 
    On average, it makes $\Psi_G(Y) + O\left(\sqrt{|V|\Psi_G(Y)} + |V|\right)$ mistakes.

  \item 
    On the lower side,
    % Up to this $O\left(\sqrt{|V|\Psi_G(Y)} + |V|\right)$ factor, it is optimal as we can show that 
    for any directed graph $G$ and any integer $K$,
    there exists a labeling $Y$ forcing at least $\frac{K}{2}$ mistakes to any online algorithms,
    while $\Psi_G(Y) \leq K$.
\end{itemize}

\end{frame}

%% PART Experiments

\section{Experiments}\label{experiments}
  \begin{frame} \frametitle{Outline} \tableofcontents[currentsection] \end{frame}

\begin{frame}{5 Real world datasets}

\begin{description}
  % \tightlist
\item[\aut{}]
\(i\) the work of \(j\) to praise it or criticise it.

\item[\wik{}]
\(i\) vote for or against \(j\) promotion to adminship.

\item[\sla{}]
\(i\) consider \(j\) as a friend or foe.

\item[\epi{}]
\(i\) trust or not the reviews made by \(j\).

\item[\kiw{}]
\(i\) reacted to a Wikipedia edit made by \(j\), to enhance it or revert it.

\end{description}

These datasets are severely unbalanced toward the positive class.
Hence, we evaluate using the Matthews Correlation Coefficient (MCC):
\[\mathrm{MCC} = \frac{tp\times tn-fp\times fn} {\sqrt{ (tp + fp) ( tp + fn ) ( tn + fp ) ( tn + fn ) } }
 \begin{dcases*}
1 & all predictions correct \\
0 & random predictions \\
-1 & all predictions incorrect \\
\end{dcases*}
\]

\end{frame}


% \begin{frame}{Protocol}
%
% We sample \(\Etrain\) uniformly at random 10 times and predict the sign of the remaining edges. 

% We evaluate using the Matthews Correlation Coefficient (MCC)

% \[\mathrm{MCC} = \frac{tp\times tn-fp\times fn} {\sqrt{ (tp + fp) ( tp + fn ) ( tn + fp ) ( tn + fn ) } }\]
%
% \end{frame}

\begin{frame}{Our methods}
\begin{itemize}
  \largelist
\item
  Our global \alert{label propagation algorithm} (called \uslpropGsec{} here), with a
  threshold set by cross-validation on \(\Etrain{}\).
\item
  We also exploit
  $$ \yhat(i,j)=\sgn\Big(\outqt{\big(1-\htr(i)\big)} + \inqt{\big(1-\hun(j)\big)} -\biasqt{\tau} -
  \tfrac{1}{2} \Big)$$
  in a passive context by computing
  \(\htr(i)\) and \(\hun(i)\), and estimating \(\tau\), on the training set \(\Etrain\).
  We call this method \usrule{} (Bayes Learning Classifier based on \emph{tr}ollness
  and \emph{un}trustworthiness).
\item
  For reference, \alert{a logistic regression model} where each edge \((i,j)\) is associated
  with the features \([1-\htr(i),  1-\hun(j)]\) computed on \(\Etrain\)
  (\uslogregp{}).
\end{itemize}
\end{frame}

\begin{frame}{Competitors}
  \begin{itemize}
    \largelist
  \item \textbf{Global}
    \begin{itemize}
    \largelist
      \item
        A logistic regression model built on \compranknodes{} scores computed
        with a PageRank-inspired algorithm tailored to directed graphs with
        negative edges~\autocite{wu2016troll}.
      \item
        A global \complowrank{} matrix completion
        method, assuming that the adjacency matrix is a partial observation of an underlying
        complete graph with $k$ clusters~\autocite{lowrankcompletion14}.
    \end{itemize}
  \item \textbf{Local}
    \begin{itemize}
    \largelist
      \item
        A logistic regression model built on a high number of so-called ``\compbayesian{}''
        features defined by~\autocite{Bayesian15}.
      \item
        A logistic regression model built on \comptriads{} features, as signed graphs exhibit
        specific triangle patterns according to the status theory~\autocite{leskovec2010}.
    \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Results}

\begin{table}[p]
  \centering
  \caption{$100\times$ MCC results on \epi{} as $|\Etrain|$ grows}
  \begin{tabular}{lcccccc|r}
    \toprule
    {}               & Global     & 3\%             & 9\%                & 15\%               & 20\%               & 25\%               & time (ms)     \\
    \midrule
    \uslogregp{}     &            & 43.51           & 54.85              & 59.29              & 61.45              & 62.95              & 32            \\
    \rowcolor{lightyellow}
    \usrule{}        &            & 41.39           & 53.23              & 57.76              & 60.06              & 61.93              & \textbf{7}    \\
    \rowcolor{lightyellow}
    \uslpropGsec{}   & \checkmark & \vsecond{51.47} & \vsecondSig{58.43} & \vsecondSig{61.41} & \vsecondSig{63.14} & \vsecondSig{64.47} & \textbf{1226} \\
    \midrule
    \compranknodes{} & \checkmark & \vfirst{52.04}  & \vfirstSig{60.21}  & \vfirstSig{62.69}  & \vfirstSig{64.13}  & \vfirstSig{65.22}  & \textbf{2341} \\
    \complowrank{}   & \checkmark & 36.84           & 43.95              & 48.61              & 51.43              & 54.51              & 121530        \\
    \compbayesian{}  &            & 31.00           & 48.24              & 56.88              & 61.49              & 64.45              & 116838        \\
    \comptriads{}    &            & 34.42           & 49.94              & 54.56              & 56.96              & 58.73              & 129           \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[p]
  \centering
  \caption{$100\times$ MCC results on \aut{} as $|\Etrain|$ grows}
    \begin{tabular}{lcccccc|r}
    \toprule
    {}               & Global     & 3\%                & 9\%                & 15\%               & 20\%               & 25\%               & time (ms)            \\
    \midrule
    \uslogregp{}     &            & \vsecondSig{15.19} & \vsecondSig{26.46} & 32.98              & 36.57              & 39.90              & 2                    \\
    \rowcolor{lightyellow}
    \usrule{}        &            & 15.09              & 26.40              & \vsecondSig{32.98} & \vsecondSig{36.72} & 40.16              & \textbf{\textless 1} \\
    \rowcolor{lightyellow}
    \uslpropGsec{}   & \checkmark & \vfirstSig{19.00}  & \vfirstSig{30.25}  & \vfirstSig{35.73}  & \vfirstSig{38.53}  & \vfirstSig{41.32}  & \textbf{16}          \\
    \midrule
    \compranknodes{} & \checkmark & 12.28              & 24.44              & 31.03              & 34.57              & 38.26              & \textbf{128}         \\
    \complowrank{}   & \checkmark & 8.85               & 17.08              & 22.57              & 25.57              & 29.24              & 1894                 \\
    \compbayesian{}  &            & 10.91              & 23.75              & 32.25              & 36.52              & \vsecondSig{40.32} & 5398                 \\
    \comptriads{}    &            & 8.62               & 16.42              & 22.01              & 24.77              & 27.13              & 5                    \\
    \bottomrule
    \end{tabular}
\end{table}

\end{frame}

\begin{frame}{Results comments}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\largelist
\item
  \alert{Global methods} outperform our local one, however they \alert{are much
  slower}, preventing them to scale to larger graphs.
\item
  Our global method \uslpropGsec{} is very competitive in terms of MCC
  performance in the \textbf{small training set} regime while being faster.
\item
  Our Bayes approximator \usrule{} closely mirrors a more involved
  \uslogregp{} model, making its training useless. Moreover, the learned weights of trollness and
  trustworthiness are almost equal across all datasets.
\end{enumerate}

\end{frame}

\section{Conclusion}\label{conclusion}
  \begin{frame} \frametitle{Outline} \tableofcontents[currentsection] \end{frame}

\begin{frame}{Discussion}

We presented two methods to perform edge sign prediction in Directed Signed
Social Networks. Both are derived from a simple generative model of edge
sign.

\alert{\usrule{} is local}, thus scalable, and although it requires a large training
set to meet its theoretical guarantees\footnote{i.e.~being Bayes optimal
w.h.p.~for all edges simultaneously.}, it works well in practice.

\alert{\uslpropGsec{} is global} yet faster than state of the art
methods while enjoying competitive performance and relying on the same
theoretical foundations.

\end{frame}
\begin{frame}{Future work}
Further directions include:

\begin{itemize}
  \largelist
  \item
    Maximizing the utility of a limited query budget in active setting.
  \item
    Extending the generative model to weighted graph.
  \item
    Designing an adaptive query strategy.
  \item
    Exploiting side information.
\end{itemize}

\end{frame}


% \section*{References}\label{references}
% \addcontentsline{toc}{section}{References}
\begin{frame}[allowframebreaks]{References}
  \printbibliography
\end{frame}

\begin{frame}[plain,c]
%\frametitle{A first slide}

\begin{center}
\Huge Thank you! \\ Questions?
\end{center}

\end{frame}

\appendix

\begin{frame}{Bias term derivation}

The in- and out-neighborhood of $i$ is denoted by $\Nout(i)$ and $\Nin(i)$,
along with degree quantities:
$\dout(i)=|\Nout(i)|=\dout^-(i)+\dout^+(i)$,\\
$\din(i)=|\Nin(i)|=\din^-(i)+\din^+(i)$.
  
\(1-\htr(i)\) = \(\frac{\hdout^-(i)}{\hdout(i)}\) is the empirical
probability of drawing a \(+1\)-labeled edge from \(\Nout(i)\), which
according to our model is
\begin{align*}
  \tfrac{1}{\dout(i)}\sum_{j\in \Nout(i)} \Pr\big( y_{i,j} = 1 \big) &=
  \tfrac{1}{\dout(i)}\sum_{j\in \Nout(i)} \frac{p_i + q_j}{2} \\ &=
\frac{1}{2}\,\Biggl(p_i + \frac{1}{\dout(i)}\sum_{j\in \Nout(i)} q_j \Biggl)=
\frac{1}{2}\,\Bigl(p_i + \qbar_i\Bigl)
\end{align*}
where \(\qbar_i\), being a
sample mean of i.i.d.~\([0,1]\)-valued random variables independently
drawn from the prior marginal \(\int_0^1 \mu\big(p,\cdot\big) dp\),
concentrates around its expectation \(\mu_q\).

The same argument for $(1-\hun(j))$ proves that
the bias term \(\tau\) is the same for all edges.
\end{frame}

\begin{frame}{Label propagation full objective}

\begin{alignat*}{5}
{\hat f}\big(\bp,\bq,{y_{i,j}}_{(i,j) \in E\setminus E_0}\big) 
&= &&\sum_{(i,j) \in E} 
\Biggl(
  \frac{1}{2}\left(\frac{1+y_{i,j}}{2}-p_i\right)^2 +
  \frac{1}{2}\left(\frac{1+y_{i,j}}{2}-q_j\right)^2 \\
  &\, &&\qquad\qquad+ \left(\frac{p_i+q_j-1}{2}\right)^2
\Biggr)\\
&= &&f_{E_0}(\bp,\bq) + f_{E\setminus E_0}(\bp,\bq, {y_{i,j}}) \\
&\, &&+ \frac{1}{2}\sum_{i\in V}
\left(\dout(i)\Bigl(p_i-\frac{1}{2}\Bigl)^2+\din(i)\Bigl(q_i-\frac{1}{2}\Bigl)^2\right) 
\end{alignat*}

\end{frame}


\begin{frame}{Online algorithm, 1. RWM node instances}

For each node \(i\), we predict the sign of edge outgoing from \(i\) by
relying on two constant experts, always predicting \(-1\) or always
predicting \(+1\). The best one will make \(\Psiout(i, Y)\) mistakes. We
combine them in a Randomized Weighted Majority algorithm (RWM) instance
associated with \(i\), call it \(RWM_{out}(i)\). The instance expected
number of mistakes is therefore~\autocite{acg02},
denoting by \(M(i,j)\) the indicator function of a mistake on edge
\((i,j)\)
\[\sum_{j \in \Nout(i)} \E\,M(i,j) = \Psiout(i,Y) + O\left(\sqrt{\Psiout(i,Y)}+ 1\right)\]

We use the same technique to predict incoming edges of each node \(j\),
the instance \(RWM_{in}(j)\) having the following average number of
mistakes
\[\sum_{i \in \Nin(j)} \E\,M(i,j) = \Psiin(j,Y) + O\left(\sqrt{\Psiin(j,Y)} + 1\right)\]

\end{frame}

\begin{frame}{Online algorithm, 2. combining instances}

We then define two meta experts: \(RWM_{out}\), which predicts
\(y_{i,j}\) as \(RWM_{out}(i)\), and \(RWM_{in}\), which predicts
\(y_{i,j}\) as \(RWM_{in}(j)\). Summing over all nodes, the number of
mistakes of these two experts satisfy

\begin{align*}
    \sum_{i \in V}\sum_{j \in \Nout(i)} \E\,M(i,j) &= \Psiout(Y) + O\left(\sqrt{|V|\Psiout(Y)} + |V|\right) \\
    \sum_{j \in V}\sum_{i \in \Nin(j)} \E\,M(i,j)  &= \Psiin(Y)  + O\left(\sqrt{|V|\Psiin(Y)}  + |V|\right)
\end{align*}
\end{frame}

\begin{frame}{Online algorithm, 3. final prediction}

Our final predictor is a RWM combination of \(RWM_{out}\) and
\(RWM_{out}\), whose expected number of mistakes is
\begin{alignat*}{3}
    \sum_{(i,j) \in E} \E\,M(i,j) 
    &= \Psi_G(Y) + O\Biggl(&&\sqrt{|V|\Psi_G(Y)} + |V| \\
  & &&+ \sqrt{\Bigl(\Psi_G(Y) + |V| + \sqrt{|V|\Psi_G(Y)}\Bigr)} \Biggr)\\
    &= \Psi_G(Y) + O\Bigl( &&\sqrt{|V|\Psi_G(Y)} + |V|\Bigr)
\end{alignat*}

\end{frame}

\begin{frame}{Datasets properties}

\begin{table}
  \centering
  % \small
  \caption{Dataset properties. \label{tab:dataset}}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Dataset & $|V|$       & $|E|$       & $\frac{|E|}{|V|}$ & $\frac{|E^+|}{|E|}$ & $\frac{\Psi_{G''}(Y)}{|E|}$ & $\frac{\Psi_G(Y)}{|E|}$ \\
    \midrule
    \aut{}  & \np{4831}   & \np{39452}  & 8.1               & 72.33\%             & .076                        & .191                    \\
    \wik{}  & \np{7114}   & \np{103108} & 14.5              & 78.79\%             & .063                        & .142                    \\
    \sla{}  & \np{82140}  & \np{549202} & 6.7               & 77.40\%             & .059                        & .143                    \\
    \epi{}  & \np{131580} & \np{840799} & 6.4               & 85.29\%             & .031                        & .074                    \\
    \kiw{}  & \np{138587} & \np{740106} & 5.3               & 87.89\%             & .034                        & .086                    \\
    \bottomrule
  \end{tabular}
\end{table}

$$\Psi_{G''}(Y) = 
\min_{\bp,\bq\in[0,1]^{|V|}}
\sum_{(i,j) \in E} \left(\frac{1+y_{i,j}}{2} - \frac{p_i+q_j}{2}\right)^2$$

\end{frame}


\begin{frame}[allowframebreaks]{Additional Results}

\begin{table}[p]
  \centering
  \caption{MCC results on \wik{} as $|\Etrain|$ grows}
\begin{tabular}{lccccc|r}
\toprule
{} &                  3\% &               9\% &              15\% &                 20\% &              25\% &   time \\
\midrule
\uslogregp{}     &  \vsecondSig{32.32}  &  \vsecond{45.57}  &   \vfirst{50.70}  &   \vfirstSig{52.98}  &   \vfirst{54.49}  &      4 \\
    \rowcolor{lightyellow}
\usrule{}        &               31.83  &            44.74  &            49.64  &               52.00  &            53.52  &      1 \\
    \rowcolor{lightyellow}
\uslpropGsec{}   &   \vfirstSig{33.92}  &   \vfirst{45.75}  &  \vsecond{50.44}  &  \vsecondSig{52.58}  &  \vsecond{54.22}  &     35 \\
    \midrule
\compranknodes{} &               26.90  &            41.60  &            48.02  &               51.42  &            53.42  &    210 \\
\compbayesian{}  &               19.94  &            38.25  &            46.82  &               50.45  &            52.78  &  14090 \\
\complowrank{}   &               19.45  &            30.75  &            35.31  &               38.16  &            39.94  &   4859 \\
\comptriads{}    &                4.29  &            24.04  &            34.42  &               38.55  &            41.51  &     11 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[p]
  \centering
  \caption{MCC results on \sla{} as $|\Etrain|$ grows}
\begin{tabular}{lccccc|r}
\toprule
{} &                  3\% &                  9\% &                 15\% &              20\% &              25\% &   time \\
\midrule
\uslogregp{}     &               32.34  &               42.16  &               46.44  &            48.71  &            50.23  &     21 \\
    \rowcolor{lightyellow}
\usrule{}        &               31.78  &               41.19  &               45.23  &            47.79  &            49.43  &      6 \\
    \rowcolor{lightyellow}
\uslpropGsec{}   &  \vsecondSig{36.62}  &  \vsecondSig{45.70}  &   \vfirstSig{49.65}  &  \vsecond{51.88}  &  \vsecond{53.30}  &    655 \\
    \midrule
\compranknodes{} &   \vfirstSig{42.90}  &   \vfirstSig{47.46}  &  \vsecondSig{48.59}  &   \vfirst{52.09}  &   \vfirst{53.46}  &   1919 \\
\compbayesian{}  &               25.11  &               37.00  &               43.28  &            47.03  &            49.46  &  77042 \\
\complowrank{}   &               34.32  &               39.42  &               41.09  &            43.10  &            44.37  &  56252 \\
\comptriads{}    &               20.95  &               39.14  &               46.27  &            49.44  &            51.51  &     78 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[p]
  \centering
  \caption{MCC results on \kiw{} as $|\Etrain|$ grows}
\begin{tabular}{lccccc|r}
\toprule
{} &                  3\% &                  9\% &                 15\% &                 20\% &                 25\% &    time \\
\midrule
\uslogregp{}     &               26.02  &  \vsecondSig{35.27}  &               38.21  &  \vsecondSig{39.58}  &  \vsecondSig{40.48}  &      28 \\
    \rowcolor{lightyellow}
\usrule{}        &  \vsecondSig{26.23}  &               35.13  &               37.72  &               38.74  &               39.48  &       7 \\
    \rowcolor{lightyellow}
\uslpropGsec{}   &   \vfirstSig{33.92}  &   \vfirstSig{38.33}  &  \vsecondSig{38.63}  &               39.16  &               39.14  &     824 \\
    \midrule
\compranknodes{} &               23.59  &               33.38  &               36.81  &               38.56  &               39.80  &    2939 \\
\compbayesian{}  &               20.02  &               33.87  &   \vfirstSig{40.14}  &   \vfirstSig{43.37}  &   \vfirstSig{45.76}  &  103264 \\
\complowrank{}   &               20.13  &               26.68  &               29.97  &               31.89  &               34.23  &  130037 \\
\comptriads{}    &                1.11  &               11.07  &               18.12  &               21.53  &               23.89  &     104 \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}
\end{document}
