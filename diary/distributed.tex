In the 90s, \textcite{Valiant1990} defined a new model of parallel computation
called bulk-synchronous parallel.

\begin{quotation}
	
Valiant observes that parallel processing has not made the expected progress in
displacing sequential processing in computationally intensive domains. He
attributes this failure to the lack of an appropriate bridging model for
parallel computation, analogous to the von Neumann model for sequential
computation. He then proposes the bulk-synchronous parallel (BSP) model as a
candidate bridging model. The key claim concerning BSP is that it provides a
fixed intermediate-level model through which high-level programs can be
efficiently mapped to diverse machine architectures. The paper supports the
claim with evidence of varying sorts: certain high-level constructs (such as
memory hashing) and algorithms are efficiently implemented in BSP, and BSP is
implemented in two low-level models. Most of the arguments are only sketched,
with details left to the references. The BSP model consists of (1) components
(each performing processing or memory functions or both), (2) a point-to-point
routing mechanism for delivering messages between components, and (3)
facilities for periodic synchronization of components. The efficiency of BSP as
a bridging model is improved by parallel slackness in programs; that is, the
number of virtual processes must be sufficiently larger than the number of
processors. Another parameter important to BSP efficiency is the ratio $g$ of
total local computation rate to total router delivery rate. Valiant states that
existing machines have $g$-values higher than ideal for BSP and investment in
communication hardware to reduce $g$ (and keep it small as machine size
increases) will result in more programmable machines. The paper is a brief,
well-written introduction to a research topic. Whether a bridging model is
needed for practical progress in parallel processing, or whether BSP is a
suitable bridging model, remains unclear. The author does not discuss evidence
against the existence of bridging models; for instance, parallel and
distributed processing form a continuum with vastly different properties from
one end to the other. Finally, one might consider the analogous failure of
nuclear fusion reactors to replace fission reactors--is something lacking in our
understanding of fusion, or is fusion simply a more difficult engineering
problem than was first expected?   

Reviewer: Ronald J. Watro 
\end{quotation}

Closer to us, the increasing size of available data has pushed companies to
innovate and Google introduced the Pregel model \autocite{Pregel10}.

While Pregel is not publicly available, several open implementation have been
described, with various modifications

\begin{itemize}
	\item Apache
		Giraph\footnote{\href{http://giraph.apache.org/}{http://giraph.apache.org/}}
	\item Mizan\footnote{Actually, I can't find its implementation} \autocite{Khayyat2013}
	\item
		GPS\footnote{\href{http://infolab.stanford.edu/gps/}{http://infolab.stanford.edu/gps/}}
		\autocite{Salihoglu2013}
\end{itemize}

\Textcite{Han2014} conducted an experimental comparaison of their performance and
usability\footnote{Along with GraphLab} on four tasks: PageRank, Single Source
Shortest Paths, Weakly Connected Components and Minimum Spanning Tree.

Another evaluation was conducted recently by \textcite{Lu2015}: \enquote{We
evaluated the performance of Giraph, GraphLab, GPS, and Pregel+. Our results
show that
Pregel+\footnote{\href{http://www.cse.cuhk.edu.hk/pregelplus/index.html}%
{http://www.cse.cuhk.edu.hk/pregelplus}} \autocite{Yan2015} and GPS have
better overall performance than Giraph and GraphLab.}

\enquote{Four frameworks - GraphLab, Apache Giraph, Giraph++ and Apache Flink -
are used to implement algorithms for the representative problems Connected
Components, Community Detection, PageRank and Clustering Coefficients.}
\autocite{Koch2016}

Besides Graph500 benchmark and its BFS kernel, \textcite{Beamer2015} introduce
more primitives and optimized baselines.

Outside the Pregel family (\emph{is it?}), we can cite:

\begin{itemize}
	\item
		GraphLab\footnote{\href{http://graphlab.org/projects/source.html}{http://graphlab.org/projects}}%
		\autocites{Low2010}{Gonzalez2012}{Low2012}{GraphLab13Low} and
		GraphChi\footnote{\href{http://graphlab.org/projects/graphchi.html}{http://graphlab.org/projects/graphchi.html}}
		\autocite{Kyrola2012}.
	\item
		Spark \autocite{Zaharia2010} and its GraphX
		extension\footnote{\href{https://spark.apache.org/graphx}{https://spark.apache.org/graphx}}.
		Yet another execution engine from Apache is Flink,  which feature a
		graph
		API\footnote{\href{http://flink.apache.org/docs/0.8/spargel_guide.html}%
			{http://flink.apache.org/docs/0.8/spargel\_guide.html}} and is the
		results of the Stratoshere\footnote{\href{http://stratosphere.eu/project/publications/}%
			{http://stratosphere.eu/project/publications/}} research project.
	\item FlashGraph\footnote{\href{http://www.cs.jhu.edu/~zhengda/\#FlashGraph}%
			{\url{http://www.cs.jhu.edu/~zhengda}}} \autocite{FlashGraph14}:
		using SSD disks instead of memory to store large graph without
		compromising too much performance.
	\item Using GPU like MapGraph\footnote{\href{http://mapgraph.io/}{http://mapgraph.io/}}
		\autocite{Fu2014},
		\textsc{Totem}\footnote{\href{http://netsyslab.ece.ubc.ca/wiki/index.php/Totem}%
			{http://netsyslab.ece.ubc.ca/wiki/index.php/Totem}}
		\autocite{Gharaibeh2013} or
		Gunrock\footnote{\href{http://gunrock.github.io/gunrock/}%
			{http://gunrock.github.io/gunrock/}} \autocite{Gunrock15}
	\item Asynchronous \autocite{Wang2013}
	\item TinkerPop \footnote{\href{http://www.tinkerpop.com/docs/3.0.0.M5/}%
			{http://www.tinkerpop.com/docs/3.0.0.M5/}}
	\item Focusing not on vertex but on neighborhood allow some algorithms to
		have smaller memory footprint and message overhead than the approaches
		above. See for instance \textsc{NScale} \autocite{Quamar2014}.
	\item Likewise, in Giraph+ \autocite{Tian2013} \enquote{instead of
		exposing the view of a single vertex to the programmers, this
		model opens up the entire subgraph of each partition to be
		programmed against}
	\item focusing on graph mining and frequent subgraph handling,
		Arabesque\footnote{\url{http://arabesque.io},
		\url{http://blog.acolyer.org/2016/01/26/arabesque/}}\autocite{Teixeira2015}
\end{itemize}

Stinger http://www.stingergraph.com/index.php?id=publications

More system in session \emph{graph processing} of HPDC'14 The 23rd International
Symposium on High-Performance Parallel and Distributed Computing 

There's been more than 80 systems proposed in the last ten years, as showed by
two surveys: \autocites{Doekemeijer2014}{McCune2015}.
(smaller survey \autocite{Aridhi2016})

As hinted by a recent paper\autocite{COST15} and another
blogpost\footnote{\href{http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html}%
	{http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html}},
one must make sure that graph's size require distributed computation, as it
comes with a non negligible overhead. Otherwise, it possible to use single
machine, parallelized algorithms.
Ligra\footnote{\href{https://github.com/jshun/ligra}%
	{https://github.com/jshun/ligra}} \autocite{Ligra13} (and its compressed
version \autocite{Ligra15}) illustrates this approach. Another approach based
on Stanford SNAP software is called Ringo\footnote{\href{http://snap.stanford.edu/ringo/}%
	{http://snap.stanford.edu/ringo/}}\autocite{Ringo15}.

On the same topic, \textcite{NativeBenchmark14} compare GraphLab, CombBLAS,
SociaLite, Galois (single node) and Giraph on the following tasks: PageRank,
Breadth First Search, Collaborative Filtering, Triangle Counting. They quantify
the overhead introduced by these framework by implementing hand tuned native
solution. These abstractions make the tasks slower by a factor of $1.1$ to
$568$ in some Giraph cases. They also propose their own framework, based on
sparse linear algebra, which alleviate these slowness by taking advantage of
processors parallelism, GraphMat \autocite{GraphMat15}.

In other cases, plain MapReduce\autocite{MapReduce04}, although it's not
specialized in graph handling can also offer competitive performances, for
instances to find Connected Components
\autocites{Kardes2014}{Qin2014}{Kiveris2014}

Then talk about related problem like edges partition \autocite{Bourse2014a},
graph partition \autocites{Partition13}{Tsourakakis2014}{Partition14}{Li2015a},
approximating the top-$k$ PageRank vertices \autocite{Mitliagkas2015},
counting small motifs \autocite{Elenberg2015} or
algorithms optimization \autocites{Salihoglu14}{Salihoglu2014b} (for instance
using algebrisation \autocite{Kaski2015}) and complexity
\autocite{ComputationBounds14,Pandurangan2015}.

Also show some recent applications, for instance in \pcc{}
\autocites{Bonchi2012}{Chierichetti2014}.
\autocite{Quick2012}
