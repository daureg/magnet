\section{Scientific work}\label{scientific-work}

\subsection{Context}\label{context}

Graphs are simple yet powerful abstractions to model relationships between
entities. Hence, they have been used to represent social networks, linked data
on the web, human cortex, scientific collaboration, physical networks like
roads or power grid, protein interaction and so on. In many cases, these graphs
adhere to the homophily assumption, meaning that nodes of the graph are
connected because they share similar properties. In social context, this is
known as ``birds of a feather flock together'' \autocite{homophily01}. Machine
Learning algorithms rely on this assumption to perform classification.

However, already in the fifties, sociologists have extended this model to take
negative relationships into account \autocites{harary1953}{Cartwright56},
namely link expressing the dissimilarity between two nodes. In social network,
this can be viewed as distrust or dislike between two users. More broadly, one
protein may hinder the action of another, and the rise of a city popularity may
negatively affect its neighbors.

Such graphs are called signed graphs and their impact on graph learning is the
subject of my thesis. More specifically, we identified several Machine Learning
graph tasks that need to be tailored to this extended model. These include
clustering, which in this context is named Correlation Clustering (CC) and
classification, whether of nodes or edges (called in that case Link
Classification).

\subsection{Problems}\label{problems}

In Correlation Clustering \autocite{Bansal2002}, given a signed graph as input,
we want to find a partition of the nodes minimizing the number of
\emph{disagreement edges} (i.e.~positive edges between clusters and negative
edges within clusters). This is useful to perform entities resolution (merging
the records from several databases that refer to the same instance using
external similarity information), to find co-references in text, to aggregate
several clusterings of the same data or to identify genes relationship.

In Link Classification \autocite{Leskovec2010}, we are also given a signed
graph, but some signs are hidden and we want to predict them, given the graph
structure and the known signs of the other edges. Examples of application
include understanding trust dynamic and communities formation, testing social
theories at a large scale and recommending products from feedback in bipartite
users/products graphs.

\subsection{State of the art}\label{state-of-the-art}

\subsubsection{Correlation Clustering}\label{correlation-clustering}

CC is a APX-hard (i.e.~there exists $c>1$ such that it is NP-hard to
approximate with a ratio of $c$). There are several lines of research to go
around this fundamental limitation:

\begin{itemize}
 \item

  approximation algorithms, which are based on Linear or Semi-definite
  Programming (for complete or general graphs respectively). The best
  approximation ratios to date are $(2.06 - \epsilon)$ for complete graphs
  \autocite{Chawla2014} and $O(\log n)$ for general graphs
  \autocite{Demaine2006}. Also worth mentioning is a simple randomized
  combinatorial algorithm (called \kwik{}) with an expected ratio of 3 for
  complete graph \autocite{Ailon2008}. On a practical side, a recent trend seems
  to be considering any graph as complete by assuming that its missing edges are
  negative.

 \item
  heuristic approaches, which do not come with theoretical guarantees
  yet provide good performances \autocites{Elsner2009}{Levorato2015}.
\end{itemize}

As current graphs can be very large, scalability becomes an issue and requires
new approaches. For instance parallel computation, which takes advantage of the
multi cores of modern hardware architecture \autocites{Pan2014}{Levorato2015}.
When the graph is too large to fit in the memory of a single machine, it is
possible to partition it and use MapReduce formalism to compute a solution in
several rounds of message passing \autocite{Chierichetti2014}. Yet another idea
is to stream the edges of the graph \autocite{Ahn2015}.

Although CC formulation is slightly different, it is also related with two
other well establish graph partitioning problems, which have been recently
extended to handle negative links: community detection
\autocites{Yang2007}{Traag2009}{Amelio2013}{Chen14} and spectral clustering
\autocites{Luca10}{Gallier15}.

\subsubsection{Link Classification}\label{link-classification}

After the work of \textcite{Leskovec2010}, who trained a logistic regression on
triangle patterns of each edge, there have been more supervised approaches
looking at higher order cycles \autocite{LowRankCompletion14}, training SVM on
graphlets (small subgraphs) \autocite{Papaoikonomou2014}, embedding the edges
in a low dimensional space \autocite{Qian2014}, or using transfer learning
\autocite{SNTransfer13}.

Departing from the batch setting, some works focus on the active scenario.
There, the learner can first select some edges --- whose signs will be revealed
--- before starting to make prediction \autocites{Cesa-Bianchi2012a}%
{Cesa-Bianchi2013}. The goal is therefore to select as few edges as possible
while minimizing the prediction error on the testing set.

\subsection{First results}\label{first-results}

I started working on CC, first by writing a state of the art, which is
summarized above. Then we studied how to transfer the combinatorial algorithm
\kwik{} to general graphs while preserving the $O(\log n)$ approximation. The
idea was to add missing edges with a sign that did not introduce bad triangles
(i.e.~triangle with a single negative edge, as such triangles always induce a
disagreement edge no matter the clustering). This dependency on triangles
proved to be costly on the running time, hurting the scalability. Furthermore,
experimental results were mixed. Namely, performances strongly varied with
respect to the order in which edges were added, in ways that we were not able
to fully explained. Therefore, we concluded that this long open standing
problem should be attacked first by focusing on interesting subclass of graphs
that have yet to be identified.

This summer, I supervised Paul Dennetiere's internship in our team. He
implemented the parallel version of \kwik{} described in \autocite{Pan2014}, as
well as a common post processing method (which merges clusters resulting in the
biggest cost function gain). This will provide a useful baseline for later
comparisons, as well as a principled starting point to improve parallelization
efficiency.

\medskip

In January, we decided to focus on Link Classification in the active setting.
Namely, we wanted to build a spanning tree $T$ of the graph and query all its
edge signs. In the two clusters case, this allow predicting the sign of $e=(i,
j) \in E$ as the product of the signs of edge along the path in $T$ from $i$ to
$j$. Defining the stretch of $T$ as $stretch = \frac{1}{|E|} \sum_{(u,v) \in E}
|path^T_{u,v}|$, it turns out that ensuring low error rate amounts to
minimizing the stretch, a long open standing problem known as Low Stretch
Spanning Tree \autocite{Abraham2012}. Although the theory is not fully ready,
experimental results show that our construction is generally competitive with a
simple yet efficient baseline and outperforms it for specific graph geometry
like grid graphs.

In March, I spent three weeks visiting Claudio Gentile at the Universita'
dell'Insubria, Varese, Italy. Professor Gentile was involved in my thesis topic
definition and has close links with our research team. We worked on a related
problem regarding similarity between the nodes of a graph across different
contexts.

\section{Roadmap}\label{roadmap}

During the second year, we plan to deepen our understanding of our problems and
our methods, by gaining theoretical and experimental insights, which could lead
to publications in international workshops or European conferences such as
ECML. In the third year, we envisioned more ambitious venues as well as
practical applications (e.g. shedding some light on massive real data through
our methods). Here are some directions along which we would like to proceed:

\kwik{} proceeds by choosing a distinguished node uniformly at random and
putting it in one cluster along with all its positive neighbors, until
exhaustion of the graph. A natural extension would be to consider larger
neighborhoods, such as nodes at distance at most 2 from the pivot. Although the
proof would be more challenging, it could reduce the number of disagreements.
Moreover, it would also be interesting to study further parallelization and
scaling issues.

In Link Classification, we mentioned that signs can be extended, going from one
binary label per edge to a more holistic approach where the similarity between
two nodes is measured across different contexts. These contexts are represented
by vectors whose dimension matches the dimension of unknown feature vectors
associated with each node. The goal is to answer query of the form: how similar
are nodes $i$ and $j$ along context $\vec{x}$. We first plan to validate the
relevance of this modelling on real problem, then test baseline methods on
synthetic and real data before looking for a more effective, online prediction
method.

\iffalse
In Link Classification, an intriguing direction in the batch supervised setting
would be to generate features using deep learning architecture
\autocites{Perozzi2014}{Yanardag2015}.
\fi

As hinted in the introduction, signed graphs are also amenable to node
classification. By leveraging our work on graph sparsifiers, we plan
to study node classification by extending the work of \textcite{Vitale2012}.
For instance, a fruitful variation of the Low Stretch Spanning Tree problem is
to consider a graph where some nodes ($X \subset V$) are distinguished (or
revealed) and try to minimize the distances between all pair of points in $X
\times \overline{X}$. In the network design community, this is known as the
Minimum Cost Routing Tree problem \autocites{Johnson1978}{Connamacher2003}.
More generally in Machine Learning, this is an instance of tree based learning
\autocite{cesa2009fast}.

I also plan to work on some follow-ups of my Master Thesis at Aalto
about mining urban data.

\section{Dissemination policy}\label{dissemination-policy}

% \emph{A statement of policy regarding results dissemination in terms of
% publications and software production}???

As said in the roadmap, my expected production consist solely of publications
in Machine Learning and Data Mining conference. In addition, we will consider
the opportunity to turn my state of the art about Correlation Clustering into a
short survey paper.

\section{Professional project}\label{professional-project}

% \emph{A section (1/2 page max) with the professional project and
% training/tracks to follow or already followed to complete this
% professional project.}

I would rather work in the industry after my PhD. This desire has been
reinforced by having taken the self evaluation guide offered by the ABG and the
university of Lille. During this year, I had the occasion to talk with members
of various companies at ICML and during my summer school, as well as with my
adviser, who spent some time in the industry. Later, I plan to apply for the
Doctoriales and if it is compatible with Inria policy, do an internship during
my third year.

However, it would be wasteful to ignore the academic environment I'm currently
working in. Therefore I have followed a training on scientific communication in
Nancy and modestly started getting involved in the community by being a
volunteer at ICML, helping for the organization of Cap and partially reviewing
two papers for NIPS. Finally I will start teaching next year, and I would like
to get some formations on this topic as well.

\vspace{4em}
\begin{center}
    Villeneuve d'Ascq, 09.09.2015 \\
    Géraud Le Falher \hspace{10em} Marc Tommasi
\end{center}
